#!/usr/bin/env python3
"""
An√°lise Detalhada da Falha dos Stories das 21h BRT
Investiga√ß√£o completa da causa raiz do problema
"""

import os
import json
import logging
from datetime import datetime, timezone, timedelta
from pathlib import Path
import sys

# Adicionar o diret√≥rio raiz ao path
sys.path.append(str(Path(__file__).parent))
sys.path.append(str(Path(__file__).parent / "src"))

class StoriesFailureAnalyzer:
    def __init__(self):
        self.setup_logging()
        self.analysis_results = {
            "timestamp": datetime.now().isoformat(),
            "analysis_type": "21h_stories_failure_investigation",
            "findings": {},
            "timeline": [],
            "evidence": [],
            "recommendations": [],
            "prevention_measures": []
        }
        
    def setup_logging(self):
        """Configurar logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
    def analyze_railway_status(self):
        """1. Analisar status atual do Railway"""
        self.logger.info("üîç 1. ANALISANDO STATUS DO RAILWAY")
        
        findings = {
            "procfile_analysis": {},
            "scheduler_configuration": {},
            "environment_variables": {},
            "discrepancies": []
        }
        
        # Analisar Procfile
        try:
            procfile_path = Path(__file__).parent / "Procfile"
            if procfile_path.exists():
                with open(procfile_path, 'r', encoding='utf-8') as f:
                    procfile_content = f.read()
                
                findings["procfile_analysis"] = {
                    "exists": True,
                    "main_command": "python railway_scheduler.py" if "railway_scheduler.py" in procfile_content else "UNKNOWN",
                    "content_preview": procfile_content[:200]
                }
                
                # DISCREP√ÇNCIA CR√çTICA IDENTIFICADA
                if "railway_automation.py" in procfile_content:
                    findings["discrepancies"].append({
                        "type": "CRITICAL_DISCREPANCY",
                        "description": "Procfile pode estar executando railway_automation.py (simula√ß√£o) em vez de railway_scheduler.py (real)",
                        "impact": "HIGH",
                        "evidence": "railway_automation.py cont√©m apenas simulate_post_creation()"
                    })
                    
        except Exception as e:
            findings["procfile_analysis"] = {"error": str(e)}
        
        # Analisar configura√ß√£o do scheduler
        try:
            scheduler_path = Path(__file__).parent / "railway_scheduler.py"
            if scheduler_path.exists():
                with open(scheduler_path, 'r', encoding='utf-8') as f:
                    scheduler_content = f.read()
                
                # Verificar agendamentos de stories
                stories_schedules = []
                if "00:00" in scheduler_content and "create_scheduled_stories" in scheduler_content:
                    stories_schedules.append("00:00 UTC (21:00 BRT)")
                if "18:00" in scheduler_content and "create_scheduled_stories" in scheduler_content:
                    stories_schedules.append("18:00 UTC (15:00 BRT)")
                if "12:00" in scheduler_content and "create_scheduled_stories" in scheduler_content:
                    stories_schedules.append("12:00 UTC (09:00 BRT)")
                
                findings["scheduler_configuration"] = {
                    "stories_schedules_found": stories_schedules,
                    "has_21h_brt_schedule": "00:00 UTC (21:00 BRT)" in stories_schedules,
                    "uses_real_generate_and_publish": "generate_and_publish" in scheduler_content,
                    "mode_parameter": "mode='stories'" in scheduler_content
                }
                
        except Exception as e:
            findings["scheduler_configuration"] = {"error": str(e)}
        
        # Verificar vari√°veis de ambiente cr√≠ticas
        env_vars_to_check = [
            'OPENAI_API_KEY', 'RAPIDAPI_KEY', 'INSTAGRAM_ACCESS_TOKEN',
            'INSTAGRAM_BUSINESS_ACCOUNT_ID', 'TELEGRAM_BOT_TOKEN', 'TELEGRAM_CHAT_ID',
            'DRY_RUN', 'SIMULATION_MODE', 'PRODUCTION_MODE', 'REAL_POSTING'
        ]
        
        for var in env_vars_to_check:
            value = os.getenv(var)
            findings["environment_variables"][var] = {
                "exists": value is not None,
                "value_preview": value[:10] + "..." if value and len(value) > 10 else value
            }
        
        # Verificar se h√° vari√°veis que for√ßam simula√ß√£o
        simulation_vars = ['DRY_RUN', 'SIMULATION_MODE', 'TEST_MODE']
        for var in simulation_vars:
            if os.getenv(var):
                findings["discrepancies"].append({
                    "type": "SIMULATION_MODE_DETECTED",
                    "description": f"Vari√°vel {var} est√° definida, pode estar for√ßando modo simula√ß√£o",
                    "impact": "HIGH",
                    "evidence": f"{var}={os.getenv(var)}"
                })
        
        self.analysis_results["findings"]["railway_status"] = findings
        
        # Adicionar √† timeline
        self.analysis_results["timeline"].append({
            "timestamp": datetime.now().isoformat(),
            "event": "Railway Status Analysis",
            "status": "COMPLETED",
            "critical_issues": len([d for d in findings["discrepancies"] if d.get("impact") == "HIGH"])
        })
        
    def analyze_telegram_integration(self):
        """2. Analisar integra√ß√£o com Telegram"""
        self.logger.info("üì± 2. ANALISANDO INTEGRA√á√ÉO COM TELEGRAM")
        
        findings = {
            "configuration": {},
            "connectivity": {},
            "silent_failures": []
        }
        
        # Verificar configura√ß√£o
        telegram_token = os.getenv('TELEGRAM_BOT_TOKEN')
        telegram_chat = os.getenv('TELEGRAM_CHAT_ID')
        
        findings["configuration"] = {
            "bot_token_configured": telegram_token is not None,
            "chat_id_configured": telegram_chat is not None,
            "token_format_valid": telegram_token and ":" in telegram_token if telegram_token else False
        }
        
        # Testar conectividade (se configurado)
        if telegram_token and telegram_chat:
            try:
                sys.path.append(str(Path(__file__).parent / "src"))
                from services.telegram_client import TelegramClient
                
                telegram_client = TelegramClient(telegram_token, telegram_chat)
                
                # Tentar enviar mensagem de teste
                test_message = f"üîç Teste de conectividade - {datetime.now().strftime('%H:%M:%S')}"
                result = telegram_client.send_message(test_message)
                
                findings["connectivity"] = {
                    "test_successful": True,
                    "test_timestamp": datetime.now().isoformat(),
                    "response": str(result)[:100]
                }
                
            except Exception as e:
                findings["connectivity"] = {
                    "test_successful": False,
                    "error": str(e),
                    "test_timestamp": datetime.now().isoformat()
                }
                
                findings["silent_failures"].append({
                    "type": "TELEGRAM_CONNECTION_FAILURE",
                    "description": "Falha na conectividade com Telegram",
                    "error": str(e),
                    "impact": "MEDIUM"
                })
        
        self.analysis_results["findings"]["telegram_integration"] = findings
        
    def analyze_instagram_publishing(self):
        """3. Analisar processo de publica√ß√£o no Instagram"""
        self.logger.info("üì∏ 3. ANALISANDO PROCESSO DE PUBLICA√á√ÉO NO INSTAGRAM")
        
        findings = {
            "credentials": {},
            "quotas": {},
            "restrictions": {},
            "api_health": {}
        }
        
        # Verificar credenciais
        instagram_token = os.getenv('INSTAGRAM_ACCESS_TOKEN')
        instagram_business_id = os.getenv('INSTAGRAM_BUSINESS_ACCOUNT_ID')
        
        findings["credentials"] = {
            "access_token_configured": instagram_token is not None,
            "business_id_configured": instagram_business_id is not None,
            "token_length": len(instagram_token) if instagram_token else 0,
            "business_id_format": instagram_business_id.isdigit() if instagram_business_id else False
        }
        
        # Verificar accounts.json
        try:
            accounts_path = Path(__file__).parent / "accounts.json"
            if accounts_path.exists():
                with open(accounts_path, 'r', encoding='utf-8') as f:
                    accounts = json.load(f)
                
                findings["credentials"]["accounts_file"] = {
                    "exists": True,
                    "account_count": len(accounts),
                    "accounts": [acc.get('nome', 'UNNAMED') for acc in accounts]
                }
                
                # Verificar se as credenciais do ambiente batem com accounts.json
                for acc in accounts:
                    if acc.get('instagram_access_token') == instagram_token:
                        findings["credentials"]["token_match_found"] = True
                        break
                else:
                    findings["credentials"]["token_match_found"] = False
                    
        except Exception as e:
            findings["credentials"]["accounts_file"] = {"error": str(e)}
        
        # Testar API do Instagram (se credenciais dispon√≠veis)
        if instagram_token and instagram_business_id:
            try:
                sys.path.append(str(Path(__file__).parent / "src"))
                from services.instagram_client_robust import InstagramClientRobust
                
                instagram_client = InstagramClientRobust(instagram_business_id, instagram_token)
                
                # Testar conectividade b√°sica
                # Nota: N√£o vamos fazer chamadas reais para evitar consumir quota
                findings["api_health"] = {
                    "client_initialized": True,
                    "credentials_format_valid": True,
                    "test_timestamp": datetime.now().isoformat()
                }
                
            except Exception as e:
                findings["api_health"] = {
                    "client_initialized": False,
                    "error": str(e),
                    "test_timestamp": datetime.now().isoformat()
                }
        
        self.analysis_results["findings"]["instagram_publishing"] = findings
        
    def analyze_ltm_logs(self):
        """4. Consultar LTM (Logs, Traces e M√©tricas)"""
        self.logger.info("üìä 4. CONSULTANDO LTM - LOGS, TRACES E M√âTRICAS")
        
        findings = {
            "log_files": {},
            "anomalous_events": [],
            "performance_metrics": {},
            "api_failures": [],
            "critical_timeframe_analysis": {}
        }
        
        # Analisar arquivos de log e relat√≥rios existentes
        log_files_to_check = [
            "simulation_issue_diagnosis_*.json",
            "comprehensive_performance_documentation_*.json",
            "final_system_verification_report_*.json",
            "connection_auth_report_*.json"
        ]
        
        project_root = Path(__file__).parent
        
        for pattern in log_files_to_check:
            matching_files = list(project_root.glob(pattern))
            if matching_files:
                latest_file = max(matching_files, key=lambda f: f.stat().st_mtime)
                try:
                    with open(latest_file, 'r', encoding='utf-8') as f:
                        content = json.load(f)
                    
                    findings["log_files"][pattern] = {
                        "file": latest_file.name,
                        "timestamp": datetime.fromtimestamp(latest_file.stat().st_mtime).isoformat(),
                        "content_summary": self._summarize_log_content(content)
                    }
                    
                    # Procurar por eventos an√¥malos
                    anomalies = self._detect_anomalies_in_log(content, latest_file.name)
                    findings["anomalous_events"].extend(anomalies)
                    
                except Exception as e:
                    findings["log_files"][pattern] = {"error": str(e)}
        
        # An√°lise espec√≠fica do timeframe cr√≠tico (21h BRT = 00h UTC)
        findings["critical_timeframe_analysis"] = {
            "target_time": "00:00 UTC (21:00 BRT)",
            "analysis_window": "23:30 UTC - 00:30 UTC",
            "events_found": [],
            "gaps_detected": []
        }
        
        # Verificar se h√° evid√™ncias de execu√ß√£o no hor√°rio cr√≠tico
        current_hour = datetime.now().hour
        if current_hour == 0 or current_hour == 23:  # Pr√≥ximo ao hor√°rio cr√≠tico
            findings["critical_timeframe_analysis"]["current_status"] = "WITHIN_CRITICAL_WINDOW"
        else:
            findings["critical_timeframe_analysis"]["current_status"] = "OUTSIDE_CRITICAL_WINDOW"
        
        self.analysis_results["findings"]["ltm_analysis"] = findings
        
    def _summarize_log_content(self, content):
        """Resumir conte√∫do de log"""
        summary = {}
        
        if isinstance(content, dict):
            # Contar chaves principais
            summary["main_keys"] = list(content.keys())[:10]  # Primeiras 10 chaves
            
            # Procurar por indicadores importantes
            if "simulation" in str(content).lower():
                summary["contains_simulation_data"] = True
            if "error" in str(content).lower():
                summary["contains_errors"] = True
            if "success" in str(content).lower():
                summary["contains_success_data"] = True
                
        return summary
        
    def _detect_anomalies_in_log(self, content, filename):
        """Detectar anomalias em logs"""
        anomalies = []
        
        content_str = str(content).lower()
        
        # Detectar simula√ß√µes quando deveria ser real
        if "simulation" in content_str and "real" not in content_str:
            anomalies.append({
                "type": "SIMULATION_MODE_DETECTED",
                "source": filename,
                "description": "Log indica modo simula√ß√£o ativo",
                "severity": "HIGH"
            })
        
        # Detectar falhas de API
        if "api" in content_str and ("error" in content_str or "fail" in content_str):
            anomalies.append({
                "type": "API_FAILURE_DETECTED",
                "source": filename,
                "description": "Poss√≠vel falha de API detectada nos logs",
                "severity": "MEDIUM"
            })
        
        # Detectar problemas de autentica√ß√£o
        if "auth" in content_str and ("invalid" in content_str or "expired" in content_str):
            anomalies.append({
                "type": "AUTHENTICATION_ISSUE",
                "source": filename,
                "description": "Poss√≠vel problema de autentica√ß√£o",
                "severity": "HIGH"
            })
        
        return anomalies
        
    def generate_recommendations(self):
        """Gerar recomenda√ß√µes baseadas na an√°lise"""
        self.logger.info("üí° GERANDO RECOMENDA√á√ïES")
        
        # Recomenda√ß√µes imediatas
        immediate_actions = []
        
        # Verificar se h√° discrep√¢ncias cr√≠ticas
        railway_findings = self.analysis_results["findings"].get("railway_status", {})
        discrepancies = railway_findings.get("discrepancies", [])
        
        for discrepancy in discrepancies:
            if discrepancy.get("impact") == "HIGH":
                if discrepancy.get("type") == "CRITICAL_DISCREPANCY":
                    immediate_actions.append({
                        "priority": "CRITICAL",
                        "action": "Verificar e corrigir Procfile para usar railway_scheduler.py",
                        "description": "Procfile pode estar executando arquivo de simula√ß√£o",
                        "estimated_time": "5 minutos"
                    })
                elif discrepancy.get("type") == "SIMULATION_MODE_DETECTED":
                    immediate_actions.append({
                        "priority": "HIGH",
                        "action": "Remover ou corrigir vari√°veis de ambiente que for√ßam simula√ß√£o",
                        "description": f"Vari√°vel detectada: {discrepancy.get('evidence')}",
                        "estimated_time": "2 minutos"
                    })
        
        # Recomenda√ß√µes de preven√ß√£o
        prevention_measures = [
            {
                "measure": "Implementar monitoramento de modo de opera√ß√£o",
                "description": "Adicionar verifica√ß√£o autom√°tica se o sistema est√° em modo real ou simula√ß√£o",
                "implementation": "Criar script de verifica√ß√£o que roda a cada hora"
            },
            {
                "measure": "Adicionar logs detalhados para stories",
                "description": "Incluir logs espec√≠ficos para publica√ß√£o de stories com timestamps",
                "implementation": "Modificar railway_scheduler.py para incluir logs detalhados"
            },
            {
                "measure": "Criar sistema de alertas para falhas silenciosas",
                "description": "Notificar via Telegram quando stories n√£o s√£o publicados no hor√°rio esperado",
                "implementation": "Implementar verifica√ß√£o p√≥s-execu√ß√£o com timeout"
            }
        ]
        
        self.analysis_results["recommendations"] = immediate_actions
        self.analysis_results["prevention_measures"] = prevention_measures
        
    def generate_technical_report(self):
        """Gerar relat√≥rio t√©cnico detalhado"""
        self.logger.info("üìã GERANDO RELAT√ìRIO T√âCNICO")
        
        # Calcular score de criticidade
        critical_issues = 0
        high_issues = 0
        medium_issues = 0
        
        for finding_category in self.analysis_results["findings"].values():
            if isinstance(finding_category, dict):
                discrepancies = finding_category.get("discrepancies", [])
                for disc in discrepancies:
                    impact = disc.get("impact", "LOW")
                    if impact == "CRITICAL":
                        critical_issues += 1
                    elif impact == "HIGH":
                        high_issues += 1
                    elif impact == "MEDIUM":
                        medium_issues += 1
        
        # Determinar status geral
        if critical_issues > 0:
            overall_status = "CRITICAL_ISSUES_FOUND"
        elif high_issues > 0:
            overall_status = "HIGH_PRIORITY_ISSUES_FOUND"
        elif medium_issues > 0:
            overall_status = "MEDIUM_PRIORITY_ISSUES_FOUND"
        else:
            overall_status = "NO_MAJOR_ISSUES_DETECTED"
        
        # Adicionar resumo executivo
        self.analysis_results["executive_summary"] = {
            "overall_status": overall_status,
            "critical_issues_count": critical_issues,
            "high_priority_issues_count": high_issues,
            "medium_priority_issues_count": medium_issues,
            "analysis_completion_time": datetime.now().isoformat(),
            "primary_cause_hypothesis": self._determine_primary_cause()
        }
        
    def _determine_primary_cause(self):
        """Determinar causa prim√°ria baseada na an√°lise"""
        railway_findings = self.analysis_results["findings"].get("railway_status", {})
        discrepancies = railway_findings.get("discrepancies", [])
        
        # Verificar se h√° discrep√¢ncia cr√≠tica no Procfile
        for disc in discrepancies:
            if disc.get("type") == "CRITICAL_DISCREPANCY":
                return "PROCFILE_EXECUTING_SIMULATION_INSTEAD_OF_REAL_SCHEDULER"
        
        # Verificar se h√° vari√°veis de simula√ß√£o
        for disc in discrepancies:
            if disc.get("type") == "SIMULATION_MODE_DETECTED":
                return "ENVIRONMENT_VARIABLES_FORCING_SIMULATION_MODE"
        
        # Se n√£o h√° discrep√¢ncias cr√≠ticas, pode ser problema de configura√ß√£o
        scheduler_config = railway_findings.get("scheduler_configuration", {})
        if not scheduler_config.get("has_21h_brt_schedule", False):
            return "MISSING_21H_BRT_SCHEDULE_CONFIGURATION"
        
        return "UNKNOWN_REQUIRES_DEEPER_INVESTIGATION"
        
    def save_report(self):
        """Salvar relat√≥rio"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"stories_21h_failure_analysis_{timestamp}.json"
        filepath = Path(__file__).parent / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"üìÑ Relat√≥rio salvo: {filename}")
        
        # Criar resumo em markdown
        self._create_markdown_summary(timestamp)
        
        return filename
        
    def _create_markdown_summary(self, timestamp):
        """Criar resumo em markdown"""
        filename = f"stories_21h_failure_analysis_summary_{timestamp}.md"
        filepath = Path(__file__).parent / filename
        
        summary = self.analysis_results["executive_summary"]
        
        markdown_content = f"""# An√°lise de Falha - Stories 21h BRT

## Resumo Executivo

**Status Geral:** {summary["overall_status"]}
**Timestamp:** {summary["analysis_completion_time"]}
**Hip√≥tese da Causa Prim√°ria:** {summary["primary_cause_hypothesis"]}

### Contadores de Problemas
- üî¥ Cr√≠ticos: {summary["critical_issues_count"]}
- üü° Alta Prioridade: {summary["high_priority_issues_count"]}
- üü† M√©dia Prioridade: {summary["medium_priority_issues_count"]}

## Recomenda√ß√µes Imediatas

"""
        
        for rec in self.analysis_results["recommendations"]:
            markdown_content += f"""### {rec['priority']} - {rec['action']}
**Descri√ß√£o:** {rec['description']}
**Tempo Estimado:** {rec['estimated_time']}

"""
        
        markdown_content += """## Medidas de Preven√ß√£o

"""
        
        for measure in self.analysis_results["prevention_measures"]:
            markdown_content += f"""### {measure['measure']}
**Descri√ß√£o:** {measure['description']}
**Implementa√ß√£o:** {measure['implementation']}

"""
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        self.logger.info(f"üìÑ Resumo em markdown salvo: {filename}")
        
    def run_complete_analysis(self):
        """Executar an√°lise completa"""
        self.logger.info("üîç INICIANDO AN√ÅLISE COMPLETA DA FALHA DOS STORIES 21H")
        
        try:
            # 1. Analisar Railway
            self.analyze_railway_status()
            
            # 2. Analisar Telegram
            self.analyze_telegram_integration()
            
            # 3. Analisar Instagram
            self.analyze_instagram_publishing()
            
            # 4. Analisar LTM
            self.analyze_ltm_logs()
            
            # 5. Gerar recomenda√ß√µes
            self.generate_recommendations()
            
            # 6. Gerar relat√≥rio t√©cnico
            self.generate_technical_report()
            
            # 7. Salvar relat√≥rio
            report_filename = self.save_report()
            
            self.logger.info("‚úÖ AN√ÅLISE COMPLETA FINALIZADA")
            self.logger.info(f"üìÑ Relat√≥rio: {report_filename}")
            
            # Mostrar resumo no console
            summary = self.analysis_results["executive_summary"]
            self.logger.info(f"üéØ CAUSA PRIM√ÅRIA IDENTIFICADA: {summary['primary_cause_hypothesis']}")
            self.logger.info(f"üî¥ Problemas cr√≠ticos: {summary['critical_issues_count']}")
            self.logger.info(f"üü° Problemas alta prioridade: {summary['high_priority_issues_count']}")
            
            return report_filename
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro durante an√°lise: {e}")
            raise

def main():
    """Fun√ß√£o principal"""
    analyzer = StoriesFailureAnalyzer()
    return analyzer.run_complete_analysis()

if __name__ == "__main__":
    main()